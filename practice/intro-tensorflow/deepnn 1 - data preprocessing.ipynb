{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules loaded.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "print(\"All modules loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uncompress_features_labels(file):\n",
    "    \"\"\"\n",
    "    Uncompress features and labels from a zip file\n",
    "    :param file: The zip file to extract the data from\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with ZipFile(file) as zipf:\n",
    "        # Progress Bar\n",
    "        filenames_pbar = tqdm(zipf.namelist(), unit='files')\n",
    "        \n",
    "        # Get features and labels from all files\n",
    "        for filename in filenames_pbar:\n",
    "            # Check if the file is a directory\n",
    "            if not filename.endswith('/'):\n",
    "                with zipf.open(filename) as image_file:\n",
    "                    image = Image.open(image_file)\n",
    "                    image.load()\n",
    "                    # Load image data as 1 dimensional array\n",
    "                    # We're using float32 to save on memory space\n",
    "                    feature = np.array(image, dtype=np.float32).flatten()\n",
    "\n",
    "                # Get the the letter from the filename.  This is the letter of the image.\n",
    "                label = os.path.split(filename)[1][0]\n",
    "\n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210001/210001 [00:39<00:00, 5303.90files/s]\n",
      "100%|██████████| 10001/10001 [00:01<00:00, 5581.01files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features.shape = (210000, 784)\n",
      "test_labels.shape = (10000,)\n",
      "train_features[0] = [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    2.    0.  164.  145.    0.    4.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    1.    0.   22.  245.  213.    9.    0.    1.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    4.    0.  119.  255.  255.\n",
      "   71.    0.    3.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.\n",
      "   12.  207.  254.  255.  165.    0.    3.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    3.    0.  104.  141.  154.  255.  233.   32.    0.    2.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    1.    0.    0.  199.   83.   64.  255.\n",
      "  255.  108.    0.    3.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    2.    0.   64.\n",
      "  209.  126.    2.  218.  255.  199.    0.    1.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    3.    0.  166.   72.  127.   27.  121.  255.  252.   58.    0.\n",
      "    3.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    2.    0.   47.  186.    0.   83.  101.   30.\n",
      "  246.  255.  146.    0.    2.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    3.    0.  145.  109.\n",
      "    0.   19.  146.    2.  180.  255.  225.   18.    0.    1.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    1.\n",
      "    0.   25.  185.   21.    0.    0.  116.   53.   77.  255.  255.   92.\n",
      "    0.    3.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    3.    0.  113.  141.    0.    4.    0.   45.  124.\n",
      "    6.  223.  255.  182.    0.    1.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    1.  187.   44.    0.\n",
      "    2.    0.    0.  136.   21.  136.  255.  243.   45.    0.    2.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    3.    0.\n",
      "   85.  164.    0.    3.    0.    3.    0.   85.   85.   38.  254.  255.\n",
      "  129.    0.    3.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    1.    0.  175.   70.    0.    2.    0.    0.    0.   15.\n",
      "  136.    0.  192.  255.  214.    7.    0.    1.    0.    0.    0.    0.\n",
      "    0.    0.    0.    3.    4.    5.    0.   58.  178.    0.    2.    3.\n",
      "    2.    2.    4.    0.  114.   46.   89.  255.  255.   79.    0.    5.\n",
      "    2.    2.    1.    0.    0.    0.    1.    1.    0.    3.    0.  146.\n",
      "   97.    0.    3.    0.    0.    0.    2.    0.   46.  102.   16.  231.\n",
      "  255.  165.    0.    2.    0.    0.    0.    0.    0.    2.    0.   77.\n",
      "  137.   82.   76.  214.   76.   59.   63.   62.   62.   62.   62.   62.\n",
      "   60.  164.    5.  151.  255.  238.   73.   59.   62.   66.   34.    0.\n",
      "    1.    0.    7.  195.  192.   99.  191.  155.   54.   73.   70.   70.\n",
      "   70.   70.   70.   72.   60.  117.   68.   49.  255.  255.  168.   56.\n",
      "   72.   75.   38.    0.    1.    0.   14.   25.    0.   11.  189.   34.\n",
      "    0.    1.    0.    0.    0.    0.    0.    1.    0.   21.  124.    5.\n",
      "  198.  255.  187.    0.    0.    1.    0.    0.    0.    0.    0.    3.\n",
      "    0.   96.  164.    0.    5.    3.    3.    3.    3.    3.    3.    3.\n",
      "    5.    0.  121.   32.  105.  255.  255.   53.    0.    5.    2.    0.\n",
      "    0.    0.    1.    3.    0.  184.  106.    0.    4.    0.    0.    0.\n",
      "    0.    0.    0.    0.    2.    0.   61.  107.   15.  242.  255.  136.\n",
      "    0.    4.    0.    0.    0.    0.    2.    0.   41.  230.   28.    0.\n",
      "    1.    0.    0.    0.    0.    0.    0.    0.    1.    0.    6.  142.\n",
      "    5.  159.  255.  204.    8.    0.    1.    0.    0.    0.    4.    0.\n",
      "  144.  219.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    3.    0.  115.   67.   63.  255.  255.   62.    0.    3.    0.\n",
      "    0.    1.    0.   20.  240.  164.    0.    2.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    1.    0.   43.  140.    0.  214.  255.\n",
      "  147.    0.    3.    0.    3.    6.    0.  116.  255.  122.    0.    6.\n",
      "    1.    0.    0.    0.    0.    0.    0.    0.    0.    1.    4.    0.\n",
      "  154.   24.  117.  255.  214.   14.    0.    4.    2.    0.   27.  226.\n",
      "  255.   95.    0.    4.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    3.    0.   90.  102.   19.  239.  255.   74.    0.    3.\n",
      "  101.   90.  163.  255.  255.  154.   87.  106.   43.    0.    1.    0.\n",
      "    0.    0.    0.    1.    0.   23.  105.   93.  118.  197.   91.  219.\n",
      "  255.  183.   92.  101.]\n"
     ]
    }
   ],
   "source": [
    "datasets_folder = '/Users/iratao/Documents/project/datasets/'\n",
    "# Get the features and labels from the zip files\n",
    "train_features, train_labels = uncompress_features_labels(datasets_folder + 'notMNIST_train.zip')\n",
    "test_features, test_labels = uncompress_features_labels(datasets_folder + 'notMNIST_test.zip')\n",
    "\n",
    "# check the shape of the data\n",
    "print('train_features.shape = {0}'.format(train_features.shape))\n",
    "print('test_labels.shape = {0}'.format(test_labels.shape))\n",
    "print('train_features[0] = {0}'.format(train_features[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Limit the amount of data to work with a docker container\n",
    "docker_size_limit = 150000\n",
    "train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max Scaling:\n",
    "$\n",
    "X'=a+{\\frac {\\left(X-X_{\\min }\\right)\\left(b-a\\right)}{X_{\\max }-X_{\\min }}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed!\n"
     ]
    }
   ],
   "source": [
    "# normalize data\n",
    "is_features_normal = False\n",
    "is_labels_encod = False\n",
    "amin = 0.1\n",
    "bmax = 0.9\n",
    "\n",
    "def normalize_data(image_data, amin, bmax):\n",
    "    image_data = np.array(image_data)\n",
    "    return amin + (image_data - np.min(image_data))*(bmax - amin) / (np.max(image_data) - np.min(image_data))\n",
    "\n",
    "# Test Cases\n",
    "np.testing.assert_array_almost_equal(\n",
    "    normalize_data(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 255]), 0.1, 0.9),\n",
    "    [0.1, 0.103137254902, 0.106274509804, 0.109411764706, 0.112549019608, 0.11568627451, 0.118823529412, 0.121960784314,\n",
    "     0.125098039216, 0.128235294118, 0.13137254902, 0.9],\n",
    "    decimal=3)\n",
    "print('Tests Passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J']\n",
      "Labels One-Hot Encoded\n"
     ]
    }
   ],
   "source": [
    "if not is_features_normal:\n",
    "    train_features = normalize_data(train_features, amin, bmax)\n",
    "    test_features = normalize_data(test_features, amin, bmax)\n",
    "    is_features_normal = True\n",
    "\n",
    "if not is_labels_encod:\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(train_labels)\n",
    "    print(encoder.classes_)\n",
    "    train_labels = encoder.transform(train_labels)\n",
    "    test_labels = encoder.transform(test_labels)\n",
    "    \n",
    "    train_labels = train_labels.astype(np.float32)\n",
    "    test_labels = test_labels.astype(np.float32)\n",
    "    is_label_encod = True\n",
    "print('Labels One-Hot Encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features and labels randomized and split.\n"
     ]
    }
   ],
   "source": [
    "# split train and validate data set\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "    train_features, train_labels, test_size=0.05, random_state=832289)\n",
    "\n",
    "print('Training features and labels randomized and split.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to pickle file...\n",
      "Data cached in pickle file\n"
     ]
    }
   ],
   "source": [
    "# save the data for easy access\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "if not os.path.isfile(pickle_file):\n",
    "    print('Saving data to pickle file...')\n",
    "    try:\n",
    "        with open(pickle_file, 'wb') as pfile:\n",
    "            pickle.dump({\n",
    "                    'train_dataset': train_features,\n",
    "                    'train_labels': train_labels,\n",
    "                    'valid_dataset': valid_features,\n",
    "                    'valid_labels': valid_labels,\n",
    "                    'test_dataset': test_features,\n",
    "                    'test_labels': test_labels,\n",
    "                }, pfile, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to ', pickle_file, ':', e )\n",
    "        raise\n",
    "print('Data cached in pickle file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
